\documentclass[10pt,twocolumn,letterpaper]{article}

% ============================================================================
% Packages
% ============================================================================
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{titlesec}

% Compact spacing
\titlespacing*{\section}{0pt}{1.5ex plus 0.5ex minus 0.2ex}{1ex plus 0.2ex}
\titlespacing*{\subsection}{0pt}{1.2ex plus 0.3ex minus 0.2ex}{0.8ex plus 0.2ex}
\setlength{\parskip}{0.3em}
\setlength{\abovecaptionskip}{5pt}
\setlength{\belowcaptionskip}{2pt}

% ============================================================================
% Title
% ============================================================================
\title{Detecting AI-Generated Videos Using Spatial and Temporal Cues}
\author{Kenaj Washington\\
Stanford University\\
\texttt{kkenajj@stanford.edu}}
\date{}

\begin{document}
\maketitle

% ============================================================================
% Abstract
% ============================================================================
\begin{abstract}
In recent years, AI video generation has improved significantly, making it difficult to distinguish videos between being real or synthetic. This is a major problem with respect to media authenticity. In this work, we explore whether frequency-domain features can help distinguish AI-generated, synthetic videos from real ones. In our exploration, we combine standard RGB frames with FFT magnitude spectra and high-frequency residual maps, feeding them into a modified ResNet-18. On a dataset of 200 videos (100 real from nateraw/kinetics, 100 synthetic from seven different generators), our best model hits 97.9\% AUC at the video level. Surprisingly, adding optical flow actually hurt performance, which subverted our expectations. Our results suggest that looking at the frequency domain catches artifacts that RGB alone misses.
\end{abstract}

% ============================================================================
% Introduction
% ============================================================================
\section{Introduction}

Video generation has come a long way in the past year alone. Tools like Sora, Stable Video Diffusion, RunwayML, and Google's Veo can now produce clips that are eerily indistinguishable from real at first glance. This may be exciting from the perspective of creativity, but this reality is accompanied with the reality of the now possible misinformation in digital media.

So far, most prior work on synthetic media detection has focused on deepfakes. More specifically, they aim to detect when someone's face has been swapped or manipulated in an otherwise real video. But with today's progress in synthetic video creation, what about videos that are entirely AI-generated? With no ``real'' version to compare with, these videos that are entirely AI-generated indeed pose a new problem.

We frame this as a binary classification task: given a short video clip, is it real (captured by a camera) or synthetic (generated by an AI model)? Our approach is to look beyond RGB pixels. Prior work on image forensics has shown that generative models often struggle with high-frequency details \cite{durall2020watch}---they produce images that look fine to humans but have telltale patterns in the frequency domain. We wanted to see if this holds for video too.

Our main findings:
\begin{itemize}[nosep]
    \item Combining RGB with FFT and residual features gives a big boost over RGB alone (+13.8\% AUC)
    \item A simple ResNet-18 with early fusion works surprisingly well
    \item Optical flow features, which we expected to help, actually made things worse
    \item The approach seems to generalize across different generators, though more testing is needed
\end{itemize}

% ============================================================================
% Related Work
% ============================================================================
\section{Related Work}

While detecting entirely synthetic videos is a developing area, there have been a few notable works that have guided our approach. Much of the early work was concerned with deepfake detection, where the main goal is to catch manipulated faces rather than entire generated scenes. Datasets like FaceForensics++ \cite{rossler2019faceforensics} and Celeb-DF \cite{li2020celeb} set up benchmarks for face-swapping detection, and Guera and Delp \cite{guera2018deepfake} showed that combining CNN and LSTM models can reveal key temporal inconsistencies. The problem with these methods, however, is that they depend on accurately locating and cropping faces. This assumption breaks down when the entire video is synthetic and a face may not be present in the generated scene.

Frequency-based forensics has shown real promise for spotting GAN-generated images. Durall et al. \cite{durall2020watch} found that GANs struggle to reproduce high-frequency components correctly. Frank et al. \cite{frank2020leveraging} showed that DCT coefficients can distinguish real from fake. Wang et al. \cite{wang2020cnn} found that CNN-generated images leave artifacts that transfer across different architectures. We took these ideas and applied them frame-by-frame to video.

AI video detection is a newer area. Chang et al. \cite{chang2024aigvdet} recently put out AIGVDet, which uses multiple forensic features including optical flow. The DeepAction dataset \cite{deepaction2024} gave us synthetic videos from multiple generators, which made our experiments possible.

% ============================================================================
% Dataset and Features
% ============================================================================
\section{Dataset and Features}

\subsection{Data Collection}

We put together 200 short video clips total:
\begin{itemize}[nosep]
    \item \textbf{Real:} 100 clips from nateraw/kinetics on Hugging Face, covering various human actions
    \item \textbf{Synthetic:} $\sim$100 clips from DeepAction \cite{deepaction2024}, spread across 7 different generators (Stable Diffusion, RunwayML, Veo, CogVideoX-5B, VideoPoet, BDAnimateDiffLightning, and Pexels AI)
\end{itemize}

We extracted and resized every 10th frame to 224$\times$224. We chose a 70/15/15 split for our train/val/test, which is stratified by class so each split has a balanced mix. One problem that we had to mend was making sure that the frames from a given video stay in the same split in order to avoid leakage. We initially made this mistake and saw surprisingly high detection accuracies because the model saw some frames from a video during training and then got tested on other frames from that same video.

\subsection{Features}

We extracted four types of features from each frame:

\textbf{RGB} (3 channels): The raw pixels, normalized to [0,1]. No manipulations here.

\textbf{FFT magnitude} (1 channel): We convert to grayscale and compute the 2D FFT:
\begin{equation}
    F(u, v) = \sum_{x,y} f(x,y) e^{-2\pi i (ux/M + vy/N)}
\end{equation}
We take the log magnitude and shift it so the DC component, the zero-frequency term that represents the average brightness of the image, is in the center. This centering makes it easier to see patterns that may radiate outward from low to high frequencies. Our reasoning here is that AI generators might leave behind fingerprints in certain frequency bands that our eyes cannot pick up on, but a trained model might.

\textbf{Residual map} (1 channel): This captures high-frequency noise patterns:
\begin{equation}
    R = |I - G_\sigma * I|
\end{equation}
where $G_\sigma$ is a Gaussian blur with $\sigma=5$. Essentially, we are blurring the original image and subtracting it from the original to isolate the remainder of the image. This decision was driven by the tendency of diffusion models to leave behind unique, characteristic denoising artifacts in these residuals.

\textbf{Optical flow} (2 channels): Catching directional and magnitudal differences between frames using Farneback's method \cite{farneback2003}. Our thinking was that this would help catch temporal inconsistencies, i.e., moments where the motion between frames doesn't quite look right and possibly don't make logical sense. Unfortunately our optimistic hypothesis didn't yield promising results in our setup.

\begin{figure*}[t]
\centering
\includegraphics[width=0.85\textwidth]{fft_comparison.png}
\caption{Example frames and their FFT spectra. Top: real video. Bottom: AI-generated. The model is able to pick up on these seemingly indiscernible differences.}
\label{fig:features}
\end{figure*}

% ============================================================================
% Methods
% ============================================================================
\section{Methods}

\subsection{Model}

For our backbone, we chose to pretrain ResNet-18 \cite{he2016deep} on ImageNet. Since we are feeding in more than just RGB information, we had to replace the first convolutional layer to accept the extra channels:
\begin{equation}
    \text{Conv2d}(C_{in}, 64, k=7, s=2, p=3)
\end{equation}
where $C_{in}$ is 3 (RGB only), 5 (RGB+FFT+residual), or 7 (if we include flow). We also swapped out the final layer for a single sigmoid output for a final binary classification.

\subsection{Training}

We trained using binary cross-entropy loss:
\begin{equation}
    \mathcal{L} = -\frac{1}{N}\sum_{i} \left[ y_i \log(\hat{y}_i) + (1-y_i)\log(1-\hat{y}_i) \right]
\end{equation}

For hyperparameters, we decided to use Adam at a learning rate of $10^{-4}$, batch size of 16, and ran training for 5 epochs. One choice we made deliberately was to skip data augmentation. Augmentations like flipping or cropping could potentially distort or destroy the forensic artifacts that our model could possibly learn from.

\subsection{Video-Level Prediction}

We train at the frame level, but when we evaluate we care about the video as a whole. By this, we mean that in order to get a video-level prediction, we take the average of the sigmoid outputs across all frames:
\begin{equation}
    P_{\text{video}} = \frac{1}{T} \sum_{t=1}^{T} \sigma(f_\theta(x_t))
\end{equation}
This helps us smooth out any noise that may appear in individual frames, resulting in a more stable prediction.

\subsection{Baseline: Logistic Regression}

To tie things back to classical machine learning, and also as an intriguing sanity check, we trained a logistic regression model on 8 hand-crafted features. We computed FFT statistics (mean, standard deviation, energy), residual statistics (mean, variance), and RGB histogram entropy. That gave us 8 features for each frame, which we then took the average across all frames in each video.

% ============================================================================
% Results
% ============================================================================
\section{Experiments and Results}

\subsection{Main Results}

Table~\ref{tab:ablation} shows the results from testing different feature combinations. One clear emerging fact is that the inclusion of forensic features made an impact.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
Model & Ch. & AUC & Acc. \\
\midrule
Logistic Reg. & 8 & 0.733 & 0.645 \\
ResNet18 (RGB) & 3 & 0.842 & 0.742 \\
ResNet18 (+FFT+Res) & 5 & \textbf{0.979} & \textbf{0.903} \\
ResNet18 (+Flow) & 7 & 0.950 & 0.774 \\
\bottomrule
\end{tabular}
\caption{Results on test set. Adding FFT and residual features boosts performance; optical flow hurts.}
\label{tab:ablation}
\end{table}

A few observations from these results:
\begin{itemize}[nosep]
    \item The deep learning models beat logistic regression by a notable margin, even when both use similar underlying features
    \item Adding FFT and residual features gives a significant $\sim$14 points of AUC improvement over RGB alone
    \item The 7-channel model with flow actually performs \textit{worse} than the 5-channel model, which we did not anticipate
\end{itemize}

\subsection{What Happened with Optical Flow?}

This was probably the biggest surprise we encountered. Going into the project, we expected optical flow to be helpful. AI-generated videos tend to contain strange motion artifacts when objects morph in unnatural ways which defy physics. Yet adding flow actually dropped our AUC by about 3 points.

\begin{figure*}[t]
\centering
\includegraphics[width=0.85\textwidth]{flow_comparison.png}
\caption{Optical flow from real vs. synthetic videos. It is visually difficult to distinguish them.}
\label{fig:flow}
\end{figure*}

Looking at Figure~\ref{fig:flow}, we can start to see why this might be the case. The flow fields from synthetic videos do not look obviously broken. Modern generators have gotten quite good at producing temporally coherent motion. We came up with a few hypotheses for what might be happening:

\begin{enumerate}[nosep]
    \item \textbf{Not enough data}: With only around 100 videos per class, the model could be fitting to spurious flow patterns
    \item \textbf{Flow estimation noise}: Farneback's algorithm is not perfect, as it introduces its own artifacts that could confuse the classifier
    \item \textbf{Generators have improved}: The temporal artifacts might be too subtle for this kind of approach, or our optical flow algorithm, to detect
    \item \textbf{Mixed-generator training}: Based on qualitative evidence of differences in videos from different generators, they likely have different temporal signatures, and training on all models might cause these patterns to cancel out and get lost
\end{enumerate}

We find hypothesis 4 to be the most interesting. Training on a single generator might allow the model to learn generator-specific temporal patterns that get diluted when you mix data from multiple sources. This is something we plan to test in future work.

\subsection{Error Analysis}

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
& Pred. Real & Pred. Synth. \\
\midrule
Actual Real & 14 & 1 \\
Actual Synth. & 2 & 14 \\
\bottomrule
\end{tabular}
\caption{Confusion matrix for 5-channel model.}
\label{tab:confusion}
\end{table}

Out of the 31 test videos, the model misclassified 3. The single false positive was a real video with unusual lighting conditions, meaning we suspect that the odd lighting triggered the residual detector since such lighting can resemble generator artifacts. The two false negatives were both from VideoPoet generator, which appears to produce particularly clean outputs that more closely resemble real footage. This observation suggests that not all generators are equally easy to detect.

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{score_hist.png}
\caption{Score distributions for real vs. synthetic videos. Good separation between classes.}
\label{fig:scores}
\end{figure}

% ============================================================================
% Discussion and Future Work
% ============================================================================
\section{Discussion}

Our findings suggest that frequency-domain forensics, which has proven effective for detecting fake images, transfers reasonably well to video. The FFT and residual features appear to be capturing something fundamental about how these generators operate. This likely relates to the upsampling and denoising steps that most generator architectures have in common.

The optical flow result was both disappointing and surprising, but it taught us that temporal analysis for AI video detection probably requires a different approach, whether it be in the optical flow algorithm itself or the training setup with respect to examples. Hand-crafted flow features aren't enough on their own. Learned temporal features, such as 3D convolutions or video transformers, might perform better, which is worth exploring in future work.

\subsection{Future Work}

We have built the infrastructure for several follow-up experiments that we plan to conduct:

\textbf{Cross-generator generalization}: Train on 6 generators and test on the 7th. This would reveal whether the model is learning something general about ``synthetic-ness'' or simply memorizing patterns specific to each generator.

\textbf{Single-generator training}: Train separate models for each generator, then revisit the flow features. If flow turns out being helpful when specializing to one generator, that would support our hypothesis about why mixed-generator training did not work.

\textbf{Scaling up}: 100 videos per class is admittedly small for a deep learning task. With more data, the flow model might start to show its value.

\textbf{Robustness testing}: What happens when videos undergo compression, resizing, or other transformations that occur when uploading to social media? Any real-world deployment would need to handle these kinds of transformations.

% ============================================================================
% Conclusion
% ============================================================================
\section{Conclusion}

We demonstrated that adding frequency-domain features (FFT and residuals) to RGB makes a meaningful difference for AI video detection. A straightforward ResNet-18 with early fusion achieves 97.9\% AUC on our dataset, which is a strong result given the limited size of the data. The unexpected finding was that optical flow actually hurt performance, suggesting that current generators produce temporally coherent content that resists motion-based detection---at least with our approach.

The takeaway is that spatial forensic features are currently exceptionally effective for this task. However, as generators continue to improve, we will likely need to develop more sophisticated approaches to temporal analysis. The ongoing competition between generators and detectors is only beginning.

% ============================================================================
% Contributions
% ============================================================================
\section*{Contributions}

This was a solo project. I was responsible for collecting and curating the dataset, building the preprocessing pipeline, training all models, running experiments, and writing this report.

% ============================================================================
% References
% ============================================================================
\bibliographystyle{plain}
\small
\begin{thebibliography}{15}

\bibitem{rossler2019faceforensics}
A. Rössler, D. Cozzolino, L. Verdoliva, C. Riess, J. Thies, and M. Nießner.
FaceForensics++: Learning to detect manipulated facial images.
In \emph{ICCV}, 2019.

\bibitem{li2020celeb}
Y. Li, X. Yang, P. Sun, H. Qi, and S. Lyu.
Celeb-DF: A large-scale challenging dataset for deepfake forensics.
In \emph{CVPR}, 2020.

\bibitem{guera2018deepfake}
D. Guera and E. J. Delp.
Deepfake video detection using recurrent neural networks.
In \emph{AVSS}, 2018.

\bibitem{durall2020watch}
R. Durall, M. Keuper, and J. Keuper.
Watch your up-convolution: CNN based generative deep neural networks are failing to reproduce spectral distributions.
In \emph{CVPR}, 2020.

\bibitem{frank2020leveraging}
J. Frank, T. Eisenhofer, L. Schönherr, A. Fischer, D. Kolossa, and T. Holz.
Leveraging frequency analysis for deep fake image recognition.
In \emph{ICML}, 2020.

\bibitem{wang2020cnn}
S. Wang, O. Wang, R. Zhang, A. Owens, and A. A. Efros.
CNN-generated images are surprisingly easy to spot... for now.
In \emph{CVPR}, 2020.

\bibitem{chang2024aigvdet}
Z. Chang et al.
AIGVDet: Detecting AI-generated videos through spatial-temporal forensics.
\emph{arXiv preprint}, 2024.

\bibitem{deepaction2024}
DeepAction Dataset.
\url{https://huggingface.co/datasets/ByteDance/DeepAction}

\bibitem{kinetics700}
nateraw/kinetics Dataset.
\url{https://huggingface.co/datasets/nateraw/kinetics}

\bibitem{he2016deep}
K. He, X. Zhang, S. Ren, and J. Sun.
Deep residual learning for image recognition.
In \emph{CVPR}, 2016.

\bibitem{farneback2003}
G. Farnebäck.
Two-frame motion estimation based on polynomial expansion.
In \emph{SCIA}, 2003.

\bibitem{pytorch}
A. Paszke et al.
PyTorch: An imperative style, high-performance deep learning library.
In \emph{NeurIPS}, 2019.

\bibitem{sklearn}
F. Pedregosa et al.
Scikit-learn: Machine learning in Python.
\emph{JMLR}, 2011.

\end{thebibliography}

\end{document}
